# Workflow: Enhance Prompt

## Objective
Transform a raw user prompt into a polished, LLM-optimized prompt using each model's official prompt engineering framework. The tool analyzes the prompt, identifies gaps, asks targeted clarifying questions (with AI-inferred example answers), and outputs a ready-to-use enhanced prompt.

## Run Command
```
streamlit run app.py
```
Opens at http://localhost:8501

## Prerequisites
- `.env` file with `ANTHROPIC_API_KEY` set (copy from `.env.example`)
- Dependencies installed: `pip install -r requirements.txt`

## Inputs

| Input | Description | Example |
|-------|-------------|---------|
| `raw_prompt` | The user's rough prompt (any length/quality) | "summarize this for my boss" |
| `target_llm` | Target model | `Claude` / `ChatGPT` / `Gemini` / `Perplexity` |
| `user_answers` | Answers to up to 4 clarifying questions (optional) | `{"context": "C-suite audience"}` |

## 4-Stage Flow

### Stage 1 — Input
User selects target LLM and pastes their raw prompt. A style hint below the selector previews what that LLM prefers.

### Stage 2 — Analysis (`analyze_prompt_components`)
One API call to `claude-haiku-4-5`. Returns which LLM-specific components are present/missing. Shows:
- Completeness progress bar
- Found components (expandable, shows extracted text)
- Missing components (listed)
- Two paths: "Enhance with current info" or "Ask me questions"

### Stage 3 — Questions (`generate_clarifying_questions`)
One API call generates up to 4 targeted questions for the most impactful missing components. Each question includes:
- An **AI-inferred example answer** drawn from the raw prompt context
- A "Use this suggestion" button to populate the text field
- Skip option for any question

### Stage 4 — Result (`build_enhanced_prompt`)
One API call builds the final prompt. Output shown in a code block with built-in copy icon. Includes a before/after expander.

## LLM Framework Summary

| LLM | Structure | Key Rules |
|-----|-----------|-----------|
| **Claude** | XML tags | Explain WHY constraints exist · Instructions after context · No filler phrases |
| **ChatGPT** | Bold headers | "Act as..." · Numbered steps · Pair examples with rules · "Think step by step." |
| **Gemini** | ## Markdown headers | Instructions LAST · Anchor phrase "Based on the above..." · Max 2–3 examples |
| **Perplexity** | Research directive | No role · No examples · No URLs · One topic · Time scope · Cite sources |

Sources: [Anthropic](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview) · [OpenAI](https://platform.openai.com/docs/guides/prompt-engineering) · [Google](https://ai.google.dev/gemini-api/docs/prompting-strategies) · [Perplexity](https://docs.perplexity.ai/guides/prompt-guide)

## API Calls Per Session

| Call | Function | Model | Tokens (approx) |
|------|----------|-------|-----------------|
| Component analysis | `analyze_prompt_components()` | claude-haiku-4-5 | ~300 in / ~150 out |
| Generate questions | `generate_clarifying_questions()` | claude-haiku-4-5 | ~400 in / ~300 out |
| Build enhanced prompt | `build_enhanced_prompt()` | claude-haiku-4-5 | ~800 in / ~600 out |

Total: ~3 calls, ~2,500 tokens per full session. Very low cost.

## Edge Cases & Known Issues

- **JSON parse failure on analysis:** Falls back to all-null dict. App continues; analysis display is skipped gracefully.
- **API key missing:** `ValueError` is caught and shown as `st.error()` with setup instructions.
- **All components already present:** `generate_clarifying_questions()` returns empty list → skips straight to result.
- **User skips all questions:** `build_enhanced_prompt()` receives empty `user_answers={}` and still produces a valid result from the analyzed components.
- **Perplexity edge case:** The build system prompt explicitly forbids adding roles, examples, or URLs even if the user's answers contain them.
- **Rate limits:** `claude-haiku-4-5` has very high throughput. Unlikely in single-user sessions. If hit, Streamlit will show the API error — simply retry.

## Notes
- All LLM differentiation is driven by `LLM_PROFILES` in `tools/enhance_prompt.py`. To refine behavior for a specific LLM, edit its `special` field.
- The `inferred_example` in questions is generated by Claude reading the raw prompt — quality depends on how much context the raw prompt contains. Short prompts will produce more generic inferences.
- The tool uses `claude-haiku-4-5-20251001` (fast, low-cost). Swap to `claude-sonnet-4-6` in `_call()` for higher quality at higher cost.
